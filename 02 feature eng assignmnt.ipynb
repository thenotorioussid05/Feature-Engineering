{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91033490-bc9b-4fcd-9998-8fe3e84ba09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52230ef9-f198-47d8-8d07-120ab1b9274e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler()\n",
      "[[0.   0.  ]\n",
      " [0.25 0.25]\n",
      " [0.5  0.5 ]\n",
      " [1.   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "#3Min-Max scaling is a data preprocessing technique that scales and translates each feature individually to a given range. It is often used to transform features to a range between zero and one on the training set¹. The transformation is given by the following formula:\n",
    "\n",
    "##[ X_{\\text{std}} = \\frac{{X - X_{\\text{min}}}}{{X_{\\text{max}} - X_{\\text{min}}}} ]\n",
    "##X_{\\text{scaled}} = X_{\\text{std}} \\times (\\text{{max}} - \\text{{min}}) + \\text{{min}} ]\n",
    "\n",
    "##where (X_{\\text{min}}) and (X_{\\text{max}}) are the minimum and maximum values of the feature, respectively¹.\n",
    "\n",
    "##This technique is often used as an alternative to zero mean, unit variance scaling¹. It does not reduce the effect of outliers but linearly scales them down into a fixed range, where the largest occurring data point corresponds to the maximum value, and the smallest one corresponds to the minimum value¹.\n",
    "\n",
    "## Here's an example to illustrate its application:\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "scaler = MinMaxScaler()\n",
    "print(scaler.fit(data))\n",
    "print(scaler.transform(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30221805-9fe9-4ea4-ae3f-e19ce41e1d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "##q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a322fa92-74fb-4a76-80fc-4ca8337c58ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.4472136   0.89442719]\n",
      " [-0.08304548  0.99654576]\n",
      " [ 0.          1.        ]\n",
      " [ 0.05547002  0.99846035]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "##The Unit Vector technique in feature scaling is a method that scales each feature individually to have a unit norm. It involves dividing each feature vector by its norm, which can be either the Manhattan distance (L1 norm) or the Euclidean distance (L2 norm) of the vector².\n",
    "\n",
    "##When scaling to the unit norm, each observation vector is divided by its norm, resulting in a feature vector with a length of one². This technique is useful when dealing with features that have hard boundaries, such as image data, where colors can range from 0 to 255⁴.\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "scaler = Normalizer(norm='l2')\n",
    "print(scaler.fit_transform(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b371aa5-de85-47c6-a8e8-3e71428b836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d09e6c3b-5c0a-48ba-9b62-b185964c550c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.05447553]\n",
      " [-3.02334666]\n",
      " [ 1.00778222]\n",
      " [ 9.07003997]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "##Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction. It identifies a set of orthogonal axes, called principal components, that capture the maximum variance in the data¹. The principal components are linear combinations of the original variables in the dataset and are ordered in decreasing order of importance¹.\n",
    "\n",
    "##PCA is widely used in exploratory data analysis and machine learning for predictive models¹. It helps reduce the dimensionality of a dataset while preserving the most important patterns or relationships between the variables without any prior knowledge of the target variables¹. By reducing the number of input features, PCA can address issues such as overfitting, increased computation time, and reduced accuracy caused by the curse of dimensionality¹.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "pca = PCA(n_components=1)\n",
    "print(pca.fit_transform(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1f6fe07-fa74-41ab-9c32-33a72e53bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "##q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31d7e1c4-f5ea-4819-9128-1bc51e9c66f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.05447553]\n",
      " [-3.02334666]\n",
      " [ 1.00778222]\n",
      " [ 9.07003997]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "##Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction¹. It identifies a set of orthogonal axes, called principal components, that capture the maximum variance in the data¹. PCA is widely used in exploratory data analysis and machine learning for predictive models¹.\n",
    "\n",
    "##Feature extraction is a process of dimensionality reduction where an initial set of raw data is reduced to more manageable groups for processing³. PCA is one of the most popular dimensionality reduction techniques used for feature extraction¹. It aims to reduce the number of input features while retaining as much of the original information as possible¹.\n",
    "\n",
    "##The relationship between PCA and feature extraction lies in the fact that PCA can be used as a technique for feature extraction¹. By transforming the original features into a new set of variables, smaller than the original set, PCA retains most of the sample's information and is useful for regression and classification tasks¹.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "pca = PCA(n_components=1)\n",
    "print(pca.fit_transform(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c8230ea-8353-4326-8685-b0026a367a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a8ccdd5-a666-4e7c-b2ea-cacacbd178af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28571429 0.77777778 0.4       ]\n",
      " [1.         0.         1.        ]\n",
      " [0.         0.44444444 0.        ]\n",
      " [0.57142857 1.         0.6       ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "##To preprocess the dataset for building a recommendation system for a food delivery service, you can use Min-Max scaling to transform the features such as price, rating, and delivery time to a specific range¹².\n",
    "\n",
    "##Min-Max scaling is a technique that scales and translates each feature individually to a given range, often between zero and one¹. It can be performed using the MinMaxScaler class from the scikit-learn library¹. The transformation is given by the following formula:\n",
    "\n",
    "##[ X_{\\text{std}} = \\frac{{X - X_{\\text{min}}}}{{X_{\\text{max}} - X_{\\text{min}}}} ]\n",
    "\n",
    "##[ X_{\\text{scaled}} = X_{\\text{std}} \\times (\\text{{max}} - \\text{{min}}) + \\text{{min}} ]\n",
    "\n",
    "##where (X_{\\text{min}}) and (X_{\\text{max}}) are the minimum and maximum values of the feature, respectively¹.\n",
    "\n",
    "##By applying Min-Max scaling to the dataset, you can ensure that each feature is transformed to a range between zero and one¹. This normalization process can help prevent features with larger values from dominating the recommendation system's calculations¹.\n",
    "\n",
    "## Here's an example of how you can use Min-Max scaling with the scikit-learn library:\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming you have a dataset with features: price, rating, and delivery time\n",
    "data = [[10.0, 4.5, 30], [15.0, 3.8, 45], [8.0, 4.2, 20], [12.0, 4.7, 35]]\n",
    "\n",
    "# Create an instance of MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the data\n",
    "scaler.fit(data)\n",
    "\n",
    "# Transform the data using Min-Max scaling\n",
    "scaled_data = scaler.transform(data)\n",
    "\n",
    "print(scaled_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b0ed484-fb5c-4935-a04f-899a851502ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "##q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0692ea2-05cb-47b5-b8ae-f35dd1bee1be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Assuming you have a dataset with multiple features\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m data \u001b[38;5;241m=\u001b[39m [[\u001b[43mfeature_1\u001b[49m, feature_2, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, feature_n], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Create an instance of PCA with desired number of components\u001b[39;00m\n\u001b[1;32m     24\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39mk)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_1' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "##Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction¹. It identifies a set of orthogonal axes, called principal components, that capture the maximum variance in the data¹. PCA is widely used in exploratory data analysis and machine learning for predictive models¹.\n",
    "\n",
    "##When working on a project to predict stock prices, you can use PCA to reduce the dimensionality of the dataset containing features such as company financial data and market trends¹.\n",
    "\n",
    "##The process of using PCA for dimensionality reduction involves the following steps:\n",
    "\n",
    "  ##  Data Preprocessing: Ensure that the dataset is properly preprocessed by handling missing values, normalizing or standardizing features, and addressing any other data quality issues.\n",
    "\n",
    "   ## Feature Selection: Identify the relevant features from the dataset that are most likely to contribute to predicting stock prices. This step helps reduce computational complexity and focuses on the most informative features.\n",
    "\n",
    "   ## Applying PCA: Apply PCA to the selected features to reduce their dimensionality while retaining most of the original information. PCA transforms the original features into a new set of uncorrelated variables called principal components¹. These principal components are ordered in decreasing order of importance, with the first component capturing the most variance in the data.\n",
    "\n",
    "   ## Determining the Number of Components: Determine the number of principal components to retain based on a trade-off between computational efficiency and information loss. You can consider using metrics such as explained variance ratio or scree plot analysis to make an informed decision¹.\n",
    "\n",
    "    ##Model Training: Train your predictive model using the reduced-dimensional dataset obtained after applying PCA. The reduced dataset should contain a subset of principal components that capture most of the variance in the original dataset.\n",
    "\n",
    "## Here's an example illustrating how you can use PCA for dimensionality reduction in Python:\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming you have a dataset with multiple features\n",
    "data = [[feature_1, feature_2, ..., feature_n], ...]\n",
    "\n",
    "# Create an instance of PCA with desired number of components\n",
    "pca = PCA(n_components=k)\n",
    "\n",
    "# Fit PCA on your dataset\n",
    "pca.fit(data)\n",
    "\n",
    "# Transform your dataset using PCA\n",
    "reduced_data = pca.transform(data)\n",
    "\n",
    "## This is just an example f n features so there is not output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c7a7f00-3f00-41b3-8e5d-072325b00511",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "323cc1db-cc77-4f7e-84f2-8bd662b53ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "Data = [[1,5,10,15,20]]\n",
    "\n",
    "min_max= MinMaxScaler()\n",
    "\n",
    "min_max.fit_transform(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "971b20c9-91df-4e65-8e9d-b1afcda9820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5b891e-eeb1-4a20-86a3-8e9b34da5f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##When performing Feature Extraction using PCA on a dataset with features such as height, weight, age, gender, and blood pressure, the number of principal components to retain depends on the desired trade-off between dimensionality reduction and information preservation¹.\n",
    "\n",
    "##To determine the number of principal components to retain, you can consider the following factors:\n",
    "\n",
    "  ##  Explained Variance Ratio: Calculate the explained variance ratio for each principal component. The explained variance ratio represents the proportion of the dataset's variance explained by each principal component. Retaining principal components with high explained variance ratios ensures that most of the original information is preserved¹.\n",
    "\n",
    "    ##Cumulative Explained Variance: Plot the cumulative explained variance ratio against the number of principal components. This plot helps visualize how much of the dataset's variance is preserved as the number of principal components increases. You can choose a threshold for the cumulative explained variance (e.g., 90% or 95%) and select the minimum number of principal components that exceed this threshold¹.\n",
    "\n",
    "    ##Domain Knowledge: Consider any domain-specific knowledge or requirements that may influence the choice of principal components to retain. For example, certain features may be more relevant or informative for your specific application, and you may want to prioritize their preservation¹.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663d9a00-6a4f-4ef3-af8b-adfb1e891341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
